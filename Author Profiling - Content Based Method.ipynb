{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author Profiling - Content Based Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "import html\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data From \"twitter-gender-corpus\" Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(preprocessing):\n",
    "    path = \"twitter-gender-corpus/*/*.txt\"\n",
    "    profiles = glob.glob(path) #get all paths from required folder\n",
    "    authors = []\n",
    "    author_type = []\n",
    "    \n",
    "    for single_profile in profiles: #append author type field 1 for \"male\" or  0\"female\"\n",
    "        if 'female' in single_profile:\n",
    "            author_type.append(0)\n",
    "        else:\n",
    "            author_type.append(1)\n",
    "        file = open(single_profile, \"r\",encoding=\"utf8\")\n",
    "        text = file.read()\n",
    "        text = html.unescape(text)\n",
    "        if(preprocessing): # do some proprocessing like removel of links and special characters\n",
    "            text = re.sub(r\"http\\S+\",\" \",text)\n",
    "            text = re.sub(r\"[^a-zA-Z0-9@#]+\",\" \",text)\n",
    "        authors.append(text)\n",
    "    return {'authors':authors,'author_type':author_type}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Word N-gram Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_n_gram(n):\n",
    "    start = time.time()\n",
    "    vector = TfidfVectorizer( # initialize tfidf vectorizer\n",
    "        stop_words='english',\n",
    "        strip_accents='unicode',\n",
    "        lowercase=True,\n",
    "        analyzer='word',\n",
    "        ngram_range=(n,n),\n",
    "        max_features=1000)\n",
    "    \n",
    "    X = vector.fit_transform(authors)\n",
    "    features = vector.get_feature_names() # features name\n",
    "    feature_attributes = X.toarray() # fearure values\n",
    "    data = pd.DataFrame(data=feature_attributes,columns=features) # convert to dataframe\n",
    "    data['author'] = author_type\n",
    "    data.to_csv('extracted-features/word_'+str(n)+'_gram.csv',index=False) # save in folder\n",
    "    end = time.time()\n",
    "    print('Exported File:','word_'+str(n)+'_gram.csv')\n",
    "    print('Execution Time:',round(end - start, 2),'sec\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Char N-gram Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_n_gram(n):\n",
    "    start = time.time()\n",
    "    vector = TfidfVectorizer( # initialize tfidf vectorizer\n",
    "        stop_words='english',\n",
    "        strip_accents='unicode',\n",
    "        lowercase=True,\n",
    "        analyzer='char',\n",
    "        ngram_range=(n,n),\n",
    "        max_features=1000)\n",
    "    \n",
    "    X = vector.fit_transform(authors)\n",
    "    features = vector.get_feature_names() # features name\n",
    "    feature_attributes = X.toarray() # fearure values\n",
    "    data = pd.DataFrame(data=feature_attributes,columns=features) # convert to dataframe\n",
    "    data['author'] = author_type\n",
    "    data.to_csv('extracted-features/char_'+str(n)+'_gram.csv',index=False) # save in folder\n",
    "    end = time.time()\n",
    "    print('Exported File:','char_'+str(n)+'_gram.csv')\n",
    "    print('Execution Time:',round(end - start, 2),'sec\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fearure Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(features,name):\n",
    "    # applying Recursive Feature Elimination to select 50 most prominenet feature based on SVM\n",
    "    # change 50 to other value if you want\n",
    "    name = name.split('.')\n",
    "    X = features.iloc[:,:-1]\n",
    "    y = features.iloc[:,-1:]\n",
    "    svm = LinearSVC()\n",
    "    rfe = RFE(svm, 50)\n",
    "    rfe = rfe.fit(X, y)\n",
    "    X[X.columns[rfe.support_]].merge(y, left_index=True, right_index=True, how='inner').to_csv('data/'+name[0]+'_reduced.csv',index=False)\n",
    "    return X[X.columns[rfe.support_]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(file,classifier):\n",
    "    # train and test classifier\n",
    "    start = time.time()\n",
    "    data = pd.read_csv('extracted-features/'+file)\n",
    "    X = feature_selection(data,file)\n",
    "    y = data.iloc[:,-1:]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=30)\n",
    "    model = eval(classifier+'()')\n",
    "    model.fit(X_train,y_train)\n",
    "    prediction = model.predict(X_test)\n",
    "    end = time.time()\n",
    "    print('File:',file)\n",
    "    print('Execution Time:',round(end - start, 2),'sec')\n",
    "    print('Accuracy: [',str(classifier),':',accuracy_score(y_test,prediction.round()).round(3),']')\n",
    "    print('RMS Error: [',str(classifier),':',mean_squared_error(y_test,prediction.round()).round(3),']\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(1) #load data with preprocessing\n",
    "authors = data['authors']\n",
    "author_type = data['author_type']\n",
    "\n",
    "# extract word 1,2,3 gram features\n",
    "word_n_gram(1)\n",
    "word_n_gram(2)\n",
    "word_n_gram(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = load_data(0) #load data without preprocessing\n",
    "authors = data['authors']\n",
    "author_type = data['author_type']\n",
    "\n",
    "# extract char 3-10 gram features\n",
    "char_n_gram(3)\n",
    "char_n_gram(4)\n",
    "char_n_gram(5)\n",
    "char_n_gram(6)\n",
    "char_n_gram(7)\n",
    "char_n_gram(8)\n",
    "char_n_gram(9)\n",
    "char_n_gram(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test mentioned algorithms on given features\n",
    "train_and_test('word_1_gram.csv','GaussianNB')\n",
    "train_and_test('word_1_gram.csv','LinearSVC')\n",
    "train_and_test('word_1_gram.csv','AdaBoostClassifier')\n",
    "train_and_test('word_1_gram.csv','RandomForestClassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test('word_2_gram.csv','GaussianNB')\n",
    "train_and_test('word_2_gram.csv','LinearSVC')\n",
    "train_and_test('word_2_gram.csv','AdaBoostClassifier')\n",
    "train_and_test('word_2_gram.csv','RandomForestClassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test('word_3_gram.csv','GaussianNB')\n",
    "train_and_test('word_3_gram.csv','LinearSVC')\n",
    "train_and_test('word_3_gram.csv','AdaBoostClassifier')\n",
    "train_and_test('word_3_gram.csv','RandomForestClassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test('char_3_gram.csv','GaussianNB')\n",
    "train_and_test('char_3_gram.csv','LinearSVC')\n",
    "train_and_test('char_3_gram.csv','AdaBoostClassifier')\n",
    "train_and_test('char_3_gram.csv','RandomForestClassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test('char_4_gram.csv','GaussianNB')\n",
    "train_and_test('char_4_gram.csv','LinearSVC')\n",
    "train_and_test('char_4_gram.csv','AdaBoostClassifier')\n",
    "train_and_test('char_4_gram.csv','RandomForestClassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test('char_5_gram.csv','GaussianNB')\n",
    "train_and_test('char_5_gram.csv','LinearSVC')\n",
    "train_and_test('char_5_gram.csv','AdaBoostClassifier')\n",
    "train_and_test('char_5_gram.csv','RandomForestClassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_and_test('char_6_gram.csv','GaussianNB')\n",
    "train_and_test('char_6_gram.csv','LinearSVC')\n",
    "train_and_test('char_6_gram.csv','AdaBoostClassifier')\n",
    "train_and_test('char_6_gram.csv','RandomForestClassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test('char_7_gram.csv','GaussianNB')\n",
    "train_and_test('char_7_gram.csv','LinearSVC')\n",
    "train_and_test('char_7_gram.csv','AdaBoostClassifier')\n",
    "train_and_test('char_7_gram.csv','RandomForestClassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test('char_8_gram.csv','GaussianNB')\n",
    "train_and_test('char_8_gram.csv','LinearSVC')\n",
    "train_and_test('char_8_gram.csv','AdaBoostClassifier')\n",
    "train_and_test('char_8_gram.csv','RandomForestClassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test('char_9_gram.csv','GaussianNB')\n",
    "train_and_test('char_9_gram.csv','LinearSVC')\n",
    "train_and_test('char_9_gram.csv','AdaBoostClassifier')\n",
    "train_and_test('char_9_gram.csv','RandomForestClassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test('char_10_gram.csv','GaussianNB')\n",
    "train_and_test('char_10_gram.csv','LinearSVC')\n",
    "train_and_test('char_10_gram.csv','AdaBoostClassifier')\n",
    "train_and_test('char_10_gram.csv','RandomForestClassifier')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
